{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pcmay/AI-Guidelines/blob/main/ALyzer3DAI_batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
        "<img src=\"https://raw.githubusercontent.com/petercmay89/ALyzer3D.AI/main/white.png\" width=\"10%\">\n",
        "<img src=\"https://raw.githubusercontent.com/petercmay89/ALyzer3D.AI/main/ALyzer3D.AI_logo.png\" width=\"25%\">\n",
        "<img src=\"https://raw.githubusercontent.com/petercmay89/ALyzer3D.AI/main/white.png\" width=\"25%\">\n",
        "<img src=\"https://raw.githubusercontent.com/petercmay89/ALyzer3D.AI/main/ColabFold_logo.png\" width=\"25%\">\n",
        "<img src=\"https://raw.githubusercontent.com/petercmay89/ALyzer3D.AI/main/white.png\" width=\"10%\">\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "Welcome to **ALyzer3D.AI BATCH**. This notebook allows you to predict the amyloidogenicity of the VL domains of a list of light chains. The tool will first generate 3D structures with [ColabFold](https://colab.research.google.com/github/sokrypton/ColabFold/blob/main/AlphaFold2.ipynb) and then automatically analyze them with the ALyzer3D.AI model.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. **Enter Your Sequences**: In the first cell (sequences_input), paste the amino acid sequences of your light chains' VL domains. Format: >ID1:sequence1;>ID2:sequence2;>ID3:sequence3;...\n",
        "2. **Select a GPU**: Click Runtime, select Change runtime type, select T4 GPU (or any GPU option available). Click Save.\n",
        "3. **Run Everything**: Click on the menu Runtime -> Run all.\n",
        "\n",
        "The notebook will now execute all the steps for you: it will install dependencies, run the ColabFold structure predictions and perform the ALyzer3D.AI analyses on the resulting top-ranked structures. At Step 3, you will be able to download a CSV file with the results.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "rWaI7kLS_CAt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "D7TpfmNIydXl"
      },
      "outputs": [],
      "source": [
        "#@title Install Dependencies and Mount\n",
        "import os\n",
        "import sys\n",
        "from sys import version_info\n",
        "\n",
        "# Check if ColabFold and its dependencies are already installed\n",
        "if not os.path.isfile(\"COLABFOLD_READY\"):\n",
        "    print(\"Installing ColabFold...\")\n",
        "    # Install ColabFold\n",
        "    os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n",
        "\n",
        "    # Fix for TensorFlow \"undefined symbol\" error\n",
        "    os.system(\"rm -f /usr/local/lib/python3.*/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so\")\n",
        "\n",
        "    # Create symbolic links\n",
        "    os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n",
        "    os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n",
        "    os.system(\"touch COLABFOLD_READY\")\n",
        "\n",
        "# Install ALyzer3D.AI dependencies\n",
        "print(\"Installing ALyzer3D.AI and its dependencies...\")\n",
        "os.system(\"git clone https://github.com/petercmay89/ALyzer3D.AI.git > /dev/null 2>&1\")\n",
        "sys.path.insert(0, '/content/ALyzer3D.AI')\n",
        "# Added biopython to the install list\n",
        "os.system(\"pip install -q transformers scikit-learn joblib biopython > /dev/null 2>&1\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Batch Prediction and Analysis\n",
        "\n",
        "#@markdown ### Paste your sequences below (format: `>ID1:SEQUENCE;>ID2:SEQUENCE`)\n",
        "sequences_input = '>ID1:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID2:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID3:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID4:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID5:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID6:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID7:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID8:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID9:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID10:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID11:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID12:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID13:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID14:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID15:DIRLTQSPSSLSASVGDRVTITCQASQHINNYLNWYQHKPGQAPKVLIYDASNLATGVPSRFSGNGSGTHFTLTINSLQPEDAATYYCQQHDDLPLTFGGGTKVEIR;>ID16:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID17:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID18:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID19:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID20:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID21:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID22:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID23:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID24:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID25:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID26:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID27:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID28:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID29:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL;>ID30:SASASLGASVNFTCTLSNEHSTYAITWHQQQPKKGPRYLMKVKSDGSHNKGDGIPDRFSGSSSGAERYLTISSLQSDNEADYYCQTWDTDILVFGGGTNLTVL' #@param {type:\"string\"}\n",
        "output_dir = 'colabfold_output'\n",
        "\n",
        "# --- Standard Parameters ---\n",
        "num_relax = 0\n",
        "template_mode = \"none\"\n",
        "msa_mode = \"mmseqs2_uniref_env\"\n",
        "model_type = \"auto\"\n",
        "pair_mode = \"unpaired_paired\"\n",
        "num_recycles = \"auto\"\n",
        "# ---------------------------\n",
        "\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import joblib\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from IPython.display import display, HTML\n",
        "from transformers import AutoTokenizer, EsmModel\n",
        "from Bio.PDB import PDBParser\n",
        "from Bio.PDB.Polypeptide import is_aa\n",
        "from Bio.Data.PDBData import protein_letters_3to1\n",
        "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "\n",
        "from colabfold.download import download_alphafold_params\n",
        "from colabfold.utils import setup_logging\n",
        "from colabfold.batch import get_queries, run, set_model_type\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1. DEFINE PREDICTOR CLASS (New Logic)\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "MODEL_FOLDER_NAME = \"paper_model_scalar_pathway_v1_minus5_stripped_80_20_seed3\"\n",
        "REPO_PATH = \"/content/ALyzer3D.AI\"\n",
        "FULL_MODEL_DIR = os.path.join(REPO_PATH, MODEL_FOLDER_NAME)\n",
        "MAX_LENGTH = 120\n",
        "PLM_MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"\n",
        "\n",
        "class AmyloidPredictor:\n",
        "    def __init__(self, model_dir):\n",
        "        self.model_dir = model_dir\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        print(f\"   -> Loading ESM-2 Model ({PLM_MODEL_NAME})...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(PLM_MODEL_NAME)\n",
        "        self.plm_model = EsmModel.from_pretrained(PLM_MODEL_NAME).to(self.device)\n",
        "        self.plm_model.eval()\n",
        "\n",
        "        print(f\"   -> Searching for models in: '{os.path.basename(model_dir)}'\")\n",
        "        self.models = []\n",
        "        self.scalers = []\n",
        "        self._load_ensemble()\n",
        "        print(\"   -> Predictor initialized.\")\n",
        "\n",
        "    def _load_ensemble(self):\n",
        "        if not os.path.exists(self.model_dir):\n",
        "            raise FileNotFoundError(f\"Directory '{self.model_dir}' does not exist.\")\n",
        "\n",
        "        model_files = sorted(glob.glob(os.path.join(self.model_dir, \"*.keras\")))\n",
        "        if not model_files:\n",
        "            model_files = sorted(glob.glob(os.path.join(self.model_dir, \"*.h5\")))\n",
        "        scaler_files = sorted(glob.glob(os.path.join(self.model_dir, \"*.joblib\")))\n",
        "\n",
        "        if not model_files or len(model_files) != len(scaler_files):\n",
        "            raise ValueError(f\"Found {len(model_files)} models and {len(scaler_files)} scalers. Mismatch or empty.\")\n",
        "\n",
        "        for mp, sp in zip(model_files, scaler_files):\n",
        "            self.models.append(tf.keras.models.load_model(mp, compile=False, safe_mode=False))\n",
        "            self.scalers.append(joblib.load(sp))\n",
        "\n",
        "    def _get_embedding(self, sequence):\n",
        "        inputs = self.tokenizer(\n",
        "            sequence, return_tensors=\"pt\", truncation=True, max_length=1022\n",
        "        ).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.plm_model(**inputs)\n",
        "        return outputs.last_hidden_state.squeeze(0).mean(dim=0).cpu().numpy()\n",
        "\n",
        "    def _calculate_rog(self, pdb_path):\n",
        "        try:\n",
        "            parser = PDBParser(QUIET=True)\n",
        "            structure = parser.get_structure(\"s\", pdb_path)\n",
        "            model = structure[0]\n",
        "            atoms = list(model.get_atoms())\n",
        "            if not atoms: return 0.0\n",
        "            com = sum(a.coord for a in atoms) / len(atoms)\n",
        "            rog_sq = sum(np.sum((a.coord - com)**2) for a in atoms)\n",
        "            n_res = len(list(model.get_residues()))\n",
        "            return np.sqrt(rog_sq / len(atoms)) / np.sqrt(n_res) if n_res > 0 else 0.0\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _get_biochem_features(self, sequence):\n",
        "        try:\n",
        "            seq = \"\".join(c for c in sequence if c in \"ACDEFGHIKLMNPQRSTVWY\")\n",
        "            pa = ProteinAnalysis(seq)\n",
        "            return [pa.isoelectric_point(), pa.gravy(), pa.aromaticity(), pa.molecular_weight()]\n",
        "        except:\n",
        "            return [7.0, 0.0, 0.0, 12000.0]\n",
        "\n",
        "    def _load_sequence(self, pdb_path):\n",
        "        parser = PDBParser(QUIET=True)\n",
        "        chain = parser.get_structure(\"s\", pdb_path)[0].get_chains().__next__()\n",
        "        return \"\".join(\n",
        "            protein_letters_3to1.get(r.get_resname().upper(), 'X')\n",
        "            for r in chain.get_residues() if is_aa(r, standard=True)\n",
        "        )\n",
        "\n",
        "    def predict(self, pdb_path, json_path):\n",
        "        try:\n",
        "            sequence = self._load_sequence(pdb_path)\n",
        "            with open(json_path, 'r') as f:\n",
        "                data = json.load(f)\n",
        "        except Exception as e:\n",
        "            return {\"error\": f\"File loading failed: {e}\"}\n",
        "\n",
        "        plddt = np.array(data['plddt'])\n",
        "        pae = np.array(data['pae'])\n",
        "\n",
        "        L_struct = min(len(sequence), len(plddt), pae.shape[0])\n",
        "        effective_len = L_struct - 5\n",
        "\n",
        "        if effective_len <= 0:\n",
        "            return {\"error\": f\"Protein too short (len={L_struct}) for -5 truncation.\"}\n",
        "\n",
        "        slice_len = min(effective_len, MAX_LENGTH)\n",
        "\n",
        "        pad_pae = np.zeros((MAX_LENGTH, MAX_LENGTH))\n",
        "        pad_pae[:slice_len, :slice_len] = pae[:slice_len, :slice_len]\n",
        "\n",
        "        pad_plddt = np.zeros(MAX_LENGTH)\n",
        "        pad_plddt[:slice_len] = plddt[:slice_len]\n",
        "\n",
        "        pad_row = np.zeros(MAX_LENGTH)\n",
        "        pad_col = np.zeros(MAX_LENGTH)\n",
        "        if slice_len > 0:\n",
        "            pad_row[:slice_len] = np.mean(pae[:slice_len, :slice_len], axis=1)\n",
        "            pad_col[:slice_len] = np.mean(pae[:slice_len, :slice_len], axis=0)\n",
        "\n",
        "        embedding = self._get_embedding(sequence)\n",
        "        biochem = self._get_biochem_features(sequence)\n",
        "        rog = self._calculate_rog(pdb_path)\n",
        "        raw_scalars = np.array(biochem + [rog]).reshape(1, -1)\n",
        "\n",
        "        inputs_base = {\n",
        "            \"pae_input\": np.expand_dims(pad_pae, [0, -1]),\n",
        "            \"plddt_input\": np.expand_dims(pad_plddt, [0, -1]),\n",
        "            \"embedding_input\": np.expand_dims(embedding, 0),\n",
        "            \"pae_row_input\": np.expand_dims(pad_row, [0, -1]),\n",
        "            \"pae_col_input\": np.expand_dims(pad_col, [0, -1]),\n",
        "            \"length_input\": np.array([effective_len])\n",
        "        }\n",
        "\n",
        "        fold_preds = []\n",
        "        for model, scaler in zip(self.models, self.scalers):\n",
        "            inputs_fold = inputs_base.copy()\n",
        "            inputs_fold[\"scalar_features_input\"] = scaler.transform(raw_scalars)\n",
        "            pred = model.predict(inputs_fold, verbose=0)[0][0]\n",
        "            fold_preds.append(pred)\n",
        "\n",
        "        avg_prob = np.mean(fold_preds)\n",
        "\n",
        "        return {\n",
        "            \"sequence\": sequence,\n",
        "            \"label\": \"AMYLOID\" if avg_prob > 0.6 else \"NON-AMYLOID\",\n",
        "            \"probability\": float(avg_prob),\n",
        "            \"fold_scores\": fold_preds\n",
        "        }\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2. MAIN BATCH EXECUTION\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# This list will store the results for Cell 3\n",
        "all_results = []\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load the AI model\n",
        "predictor = None\n",
        "try:\n",
        "    predictor = AmyloidPredictor(model_dir=FULL_MODEL_DIR)\n",
        "except Exception as e:\n",
        "    print(f\"❗️ Error loading model: {e}\")\n",
        "\n",
        "# Proceed only if the model was loaded successfully\n",
        "if predictor:\n",
        "    # Parse the input sequences\n",
        "    sequences_to_process = []\n",
        "    processed_input = sequences_input.strip()\n",
        "    if processed_input.startswith('>'):\n",
        "        processed_input = processed_input[1:]\n",
        "    entries = processed_input.split(';')\n",
        "\n",
        "    for entry in entries:\n",
        "        if ':' in entry:\n",
        "            parts = entry.split(':', 1)\n",
        "            if len(parts) == 2 and parts[0].strip() and parts[1].strip():\n",
        "                sequences_to_process.append({'id': parts[0].strip(), 'sequence': parts[1].strip()})\n",
        "            else:\n",
        "                print(f\"⚠️ Skipping malformed entry part: '{entry}'\")\n",
        "        elif entry.strip():\n",
        "            print(f\"⚠️ Skipping malformed entry (missing ':'): '{entry}'\")\n",
        "\n",
        "    print(f\"\\n✅ Found {len(sequences_to_process)} sequences to process.\")\n",
        "\n",
        "    # Process each parsed sequence\n",
        "    for item in sequences_to_process:\n",
        "        query_id = item['id']\n",
        "        query_sequence = \"\".join(item['sequence'].split())\n",
        "\n",
        "        sanitized_jobname = re.sub(r'\\W+', '', query_id)\n",
        "        jobname = f\"{output_dir}/{sanitized_jobname}\"\n",
        "        os.makedirs(jobname, exist_ok=True)\n",
        "\n",
        "        queries_path = os.path.join(jobname, f\"{sanitized_jobname}.csv\")\n",
        "        with open(queries_path, \"w\") as text_file:\n",
        "            text_file.write(f\"id,sequence\\n{sanitized_jobname},{query_sequence}\")\n",
        "\n",
        "        print(f\"\\n{'='*60}\\nProcessing ID: {query_id}\")\n",
        "\n",
        "        result_dir = Path(jobname)\n",
        "        setup_logging(result_dir.joinpath(\"log.txt\"))\n",
        "\n",
        "        queries, is_complex = get_queries(queries_path)\n",
        "        model_type_run = set_model_type(is_complex, model_type)\n",
        "        num_recycles_parsed = None if num_recycles == \"auto\" else int(num_recycles)\n",
        "\n",
        "        download_alphafold_params(model_type_run, Path(\".\"))\n",
        "\n",
        "        # Run ColabFold\n",
        "        run(\n",
        "            queries=queries, result_dir=result_dir, use_templates=(template_mode != \"none\"),\n",
        "            num_relax=num_relax, msa_mode=msa_mode, model_type=model_type_run,\n",
        "            num_models=5, num_recycles=3, model_order=[1, 2, 3, 4, 5],\n",
        "            is_complex=is_complex, data_dir=Path(\".\"), keep_existing_results=False,\n",
        "            rank_by=\"auto\", pair_mode=pair_mode, stop_at_score=100.0,\n",
        "            zip_results=False, user_agent=\"colabfold/google-colab-main\",\n",
        "        )\n",
        "\n",
        "        pdb_file = next(Path(jobname).glob(\"*_unrelaxed_rank_001*.pdb\"), None)\n",
        "        json_file = next(Path(jobname).glob(\"*_scores_rank_001*.json\"), None)\n",
        "\n",
        "        if pdb_file and json_file:\n",
        "            print(f\"   -> Analyzing structure...\")\n",
        "            result = predictor.predict(pdb_path=str(pdb_file), json_path=str(json_file))\n",
        "\n",
        "            if result.get(\"error\"):\n",
        "                print(f\"❗️ Analysis Error: {result['error']}\")\n",
        "                all_results.append({\n",
        "                    \"ID\": query_id,\n",
        "                    \"Sequence\": query_sequence,\n",
        "                    \"Prediction\": \"Error\",\n",
        "                    \"Probability\": 0.0,\n",
        "                    \"Notes\": result['error']\n",
        "                })\n",
        "            else:\n",
        "                prob = result['probability']\n",
        "                label = result['label']\n",
        "                print(f\"   -> Prediction: {label} ({prob:.4f})\")\n",
        "\n",
        "                all_results.append({\n",
        "                    \"ID\": query_id,\n",
        "                    \"Sequence\": result['sequence'],\n",
        "                    \"Prediction\": label,\n",
        "                    \"Probability\": prob,\n",
        "                    \"Notes\": \"Success\"\n",
        "                })\n",
        "        else:\n",
        "            print(f\"❗️ Error: Could not find ColabFold output files for {sanitized_jobname}.\")\n",
        "            all_results.append({\n",
        "                \"ID\": query_id, \"Sequence\": query_sequence,\n",
        "                \"Prediction\": \"Processing Error\",\n",
        "                \"Probability\": 0.0,\n",
        "                \"Notes\": \"ColabFold failed to generate output\"\n",
        "            })\n",
        "\n",
        "    print(\"\\n\\n✅ Batch processing complete.\")"
      ],
      "metadata": {
        "id": "aK0C2UpWyzFS",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Results as CSV\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Check if the 'all_results' list exists and has content\n",
        "if 'all_results' in locals() and all_results:\n",
        "    # Convert the list of dictionaries to a pandas DataFrame\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "\n",
        "    # Create a cleaner display column for confidence\n",
        "    results_df['Confidence %'] = (results_df['Probability'] * 100).round(2)\n",
        "\n",
        "    # Reorder columns slightly for readability\n",
        "    cols = ['ID', 'Prediction', 'Probability', 'Confidence %', 'Sequence', 'Notes']\n",
        "    results_df = results_df[cols]\n",
        "\n",
        "    # Define the CSV filename\n",
        "    csv_filename = 'alyzer3d_batch_results.csv'\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    results_df.to_csv(csv_filename, index=False)\n",
        "\n",
        "    print(f\"✅ Results have been saved to '{csv_filename}'.\")\n",
        "    print(\"Preview of results:\")\n",
        "    display(results_df.head())\n",
        "\n",
        "    print(\"\\nStarting download...\")\n",
        "    # Trigger the file download in the browser\n",
        "    files.download(csv_filename)\n",
        "else:\n",
        "    print(\"❗️ No results found to download. Please run the 'Batch Prediction and Analysis' cell (Cell 2) first.\")"
      ],
      "metadata": {
        "id": "RwP2xRea7GbA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}